{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfGP0RCzWbiCxciUJG4c2D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Cyclical Learning Rates for Training Neural Networks"],"metadata":{"id":"0ok12bAFl003"}},{"cell_type":"markdown","source":["Link to the [article](https://arxiv.org/pdf/1506.01186.pdf)"],"metadata":{"id":"mzkv33twmK0P"}},{"cell_type":"markdown","source":["## 1. Introduction"],"metadata":{"id":"dUF4soZOl_kg"}},{"cell_type":"markdown","source":["The method \"Cyclical learning\" means using a constantly varying between two values learning rate instead of a monotonically decreasing one. This method claims there is a significant increase in model performance compared to its alternative in metrics like accuracy. Additionally, CLRs (Cyclic Learning Rates) take essentially no additionall computation."],"metadata":{"id":"tXXsuXilmBcb"}},{"cell_type":"markdown","source":["The benefits of this method are demonstrated on the CIFAR-10, CIFAR-100 and ImageNet datasets with multiple neural networks.\n","\n","An example:\n","Classification accuracy on the CIFAR-10 with the same model architecture:\n","- Traditional learning rate: 81,4%, 70000 itearations\n","- CLR: 81,4%, 25000 itearations"],"metadata":{"id":"71dH-0OYohO9"}},{"cell_type":"markdown","source":["## 2. Related work"],"metadata":{"id":"yprwERfqqLHn"}},{"cell_type":"markdown","source":["Adaptive learning rates can be considered competitors to CLRs in terms of performance but they have a much higher computational cost. However, while adaptive learning policies are very different, adaptive learning can be combined with CLR."],"metadata":{"id":"d7uLUUZ_qRRx"}},{"cell_type":"markdown","source":["## 3. Optimal Learning Rates"],"metadata":{"id":"J9NCuSH8rDyq"}},{"cell_type":"markdown","source":["### 3.1. Cyclical Learning Rates"],"metadata":{"id":"ap3rZweRsnoY"}},{"cell_type":"markdown","source":["The premise is that increasing learning rate might have a negetive effect temporarily but a long-term beneficial effect, becuase of being able to easily traverse saddle points. So we are going to let the learning rate vary between two set boundries -`max_lr` and `base_lr`"],"metadata":{"id":"YkAGxqWztAk4"}},{"cell_type":"markdown","source":["There are multiple ways the learning rate can vary between two values:\n","- Triangular - the lr varies linearly between the boundries\n","- Welch window - the lr varies parabolically between the boundries\n","- Haan window - the lr varies sinusoidally between the boundries"],"metadata":{"id":"nj_cAb6PuOzP"}},{"cell_type":"markdown","source":["There are also two policies that include dynamic change of the boundries:\n","- Triangular2 - the lr varies linearly between the boundries, but the learning rate difference is halved at the end of each cycle\n","- Exp_range - the lr varies between boundries, which decline by an exponential factor $Î³^{iteration}$"],"metadata":{"id":"9q3iz-J_vBy-"}},{"cell_type":"markdown","source":["### 3.2. How to estimate a good value for the cycle length"],"metadata":{"id":"-a3CNInJxZOb"}},{"cell_type":"markdown","source":["Setting the stepsize to 2-10 times the number of iterations in an epoch is optimal. It is shown that replacing a constant learning rate step with 3-4 cycles yields significantly better results. It is recommended you stop training when the learning rate is at the lower boundry"],"metadata":{"id":"KaRFpWdWxhP0"}},{"cell_type":"markdown","source":["### 3.3. How to estimate reasonable minimum and maximum boundry values"],"metadata":{"id":"bJNJQqzIzDMW"}},{"cell_type":"markdown","source":["The \"LR range test\":\n","- set some boundry values\n","- run your model for several epochs while letting the lr cycle\n","- plot the accuracy (or any other metric) on the Y axis and the learning rate on the X axis\n","- locate the points where the model starts to converge and where it starts to diverge\n","- set `base_lr` and `max_lr` to these points or , alternatively, `base_lr` = $\\frac{1}{3}$ first point and `max_lr` = $\\frac{1}{4}$ second point"],"metadata":{"id":"U2w41LkczNYU"}},{"cell_type":"markdown","source":["## 4. Experiments"],"metadata":{"id":"K-IiLuPZIMhF"}},{"cell_type":"markdown","source":["This technique was tested on several datasets:"],"metadata":{"id":"_1FOUUI4IO4V"}},{"cell_type":"markdown","source":["### 4.1. CIFAR-10 and CIFAR-100 Datasets"],"metadata":{"id":"v5qUYjv4IrRG"}},{"cell_type":"markdown","source":["- Caffe CIFAR-10, Triangular2 policy - reduced number of iterations to reach optimal accuracy from 70000 to 25000\n","- Further testing the Exp_range policy - outperformed the standard exponential decay in accuracy and efficiency\n","- Applying CLR to ResNets, Stochastic Depth networks and DenseNets improved accuracy compared to their standard learning rate technique counterparts"],"metadata":{"id":"Sr2AAPihIwaB"}},{"cell_type":"markdown","source":["### 4.2. ImageNet Dataset"],"metadata":{"id":"rBvhvDipJtKI"}},{"cell_type":"markdown","source":["- AlexNet showed improvements in accuracy with fewer iterations needed\n","- Improved performance of GoogLeNet/Inception"],"metadata":{"id":"JYghLDYMKcgh"}},{"cell_type":"markdown","source":["These experiments show that CLR is definitelly affective in image-related tasks, having exceeded traditional performance metrics with reduced training time."],"metadata":{"id":"CR5HDefiKxTU"}},{"cell_type":"markdown","source":["## 5. Conclusion"],"metadata":{"id":"LvsN9uDjFGDL"}},{"cell_type":"markdown","source":["The results from the experiments confirm the benefits of CLRs - they improve performance, do not require additional computational power, easy to find the right boundries."],"metadata":{"id":"jrGubM_SFI8t"}}]}